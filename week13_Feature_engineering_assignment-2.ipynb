{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to transform numeric features to a specific range, typically between 0 and 1. It rescales the data by subtracting the minimum value from each data point and then dividing by the range (difference between maximum and minimum values). This method is particularly useful when features have varying scales and you want to ensure that they are all on a comparable scale for certain machine learning algorithms that are sensitive to feature scaling.\n",
    "\n",
    "Xscaled= (Xi-Xmin) / (Xmax-Xmin)\n",
    "\n",
    "**Example**\n",
    "\n",
    "Age: [10, 5, 15, 20] , Xmin=5, Xmax=20\n",
    "\n",
    "for X=10 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(10-5)/(20-5)=5/15=1/3\n",
    "\n",
    "for X=5 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(5-5)/(20-5)  =0\n",
    "\n",
    "for X=15 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(15-5)/(20-5)=10/15=2/3\n",
    "\n",
    "for X=20 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(20-5)/(20-5)=1\n",
    "\n",
    "The scaled data will now have values between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "The **Unit Vector technique**, also known as **Normalization**, is a feature scaling method used to rescale data so that each data point lies on the unit hypersphere. In other words, it scales each data point to have a length (norm) of 1 while preserving the direction of the original vector. This technique is useful when you want to ensure that data points have the same scale and magnitude, which can be beneficial for certain algorithms that rely on distances or dot products, like clustering or support vector machines.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "Xnormalized = X / |X|\n",
    "\n",
    "Where:\n",
    " Xnormalized is the normalized version of the original data point X \n",
    "\n",
    " |X| is the Euclidean norm (length) of the original data point X \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's use the same example of the house prices dataset with features \"Size\" and \"Age.\"\n",
    "\n",
    "Original data:\n",
    " Size: [1500, 2000, 1800, 2200]\n",
    "\n",
    " Age: [10, 5, 15, 20]\n",
    "\n",
    "1. Calculate the Euclidean norm for each data point:\n",
    "\n",
    "    For Size = 1500: |X| = sqrt(1500^2 + 10^2)\n",
    "\n",
    "    For Size = 2000: |X| = sqrt(2000^2 + 5^2)\n",
    "\n",
    "    For Size = 1800: |X| = sqrt(1800^2 + 15^2)\n",
    "\n",
    "    For Size = 2200: |X| = sqrt(2200^2 + 20^2)\n",
    "\n",
    "2. Apply Unit Vector scaling:\n",
    "\n",
    "    For Size = 1500: ( Xnormalized) = 1500\\sqrt(1500^2 + 10^2)\n",
    "\n",
    "    For Size = 2000: ( Xnormalized) = 2000\\sqrt(2000^2 + 5^2)\n",
    "\n",
    "    For Size = 1800: ( Xnormalized) = 1800\\sqrt(1800^2 + 15^2)\n",
    "    \n",
    "    For Size = 2200: ( Xnormalized) = 2200\\sqrt(2200^2 + 20^2)\n",
    "\n",
    "This process ensures that each data point's length (Euclidean norm) is 1, while their directions are preserved. Unit Vector scaling is particularly helpful when you want to focus on the relative relationships between data points rather than their absolute magnitudes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original data's variance as possible. It achieves this by identifying new orthogonal axes, called principal components, along which the data has the maximum variance.\n",
    "\n",
    "PCA works by projecting the original data onto these principal components, effectively creating a new coordinate system where the first principal component captures the most variance, the second principal component captures the second most variance, and so on. This way, a significant portion of the data's variability can be explained using fewer dimensions.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a dataset of 2D points where you want to reduce the dimensionality from 2D to 1D using PCA.\n",
    "\n",
    "Original 2D data points:\n",
    "\n",
    "(1, 2),\n",
    "(2, 3),\n",
    "(3, 4),\n",
    "(4, 5),\n",
    "(5, 6)\n",
    "\n",
    "Data Standardization: Standardize the data (mean = 0, standard deviation = 1) since PCA is sensitive to scale.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues represent the amount of variance captured by each component.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors by their corresponding eigenvalues in decreasing order. This ranks the principal components based on the amount of variance they capture.\n",
    "\n",
    "Choose Principal Components: Choose the top k eigenvectors to form a matrix. In this case, we want to reduce to 1D, so we choose the eigenvector corresponding to the highest eigenvalue.\n",
    "\n",
    "Projection: Project the standardized data onto the chosen principal component(s).\n",
    "\n",
    "The reduced 1D data points after PCA:\n",
    "\n",
    "(2.03),\n",
    "(2.99),\n",
    "(3.94),\n",
    "(4.90),\n",
    "(5.95)\n",
    "\n",
    "By projecting the original 2D data onto a single principal component, PCA has effectively reduced the dimensionality while retaining a substantial amount of the variance present in the original data. In this example, the data was reduced to 1D, but in practice, you can choose the number of principal components based on the desired level of dimensionality reduction and variance retention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "**PCA (Principal Component Analysis)** and **Feature Extraction** are closely related concepts. PCA can be used as a feature extraction technique to transform the original features into a new set of features (principal components) that capture the most important information in the data while reducing its dimensionality.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset of images where each image is represented by a vector of pixel values. Each pixel value is considered a feature. Let's say you have 100x100 pixel images, which means you have 10,000 features for each image. Using PCA, you can perform feature extraction to reduce the dimensionality of the images while retaining the most significant information.\n",
    "\n",
    "1. **Data Preparation**: Flatten each image into a 10,000-dimensional vector, resulting in a matrix where each row corresponds to an image.\n",
    "\n",
    "2. **Data Standardization**: Standardize the pixel values (mean = 0, standard deviation = 1) to ensure that all features are on a similar scale.\n",
    "\n",
    "3. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "4. **Calculate Eigenvectors and Eigenvalues**: Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues represent the amount of variance captured by each component.\n",
    "\n",
    "5. **Sort Eigenvectors**: Sort the eigenvectors by their corresponding eigenvalues in decreasing order. This ranks the principal components based on the amount of variance they capture.\n",
    "\n",
    "6. **Choose Principal Components**: Select a subset of the top-k eigenvectors to use as the new feature space. These eigenvectors represent the most important patterns in the original data.\n",
    "\n",
    "7. **Projection**: Project the standardized data onto the chosen principal components to generate the new feature vectors.\n",
    "\n",
    "By selecting a subset of the top principal components, you have effectively extracted a compressed representation of the original images. These new features are linear combinations of the original pixel values that capture the most significant variations in the data. The dimensionality of the data has been reduced, which can be especially beneficial for machine learning tasks where high-dimensional data might lead to overfitting or increased computational complexity.\n",
    "\n",
    "In this example, PCA serves as a feature extraction technique by transforming the pixel-based features into a smaller set of principal component-based features that retain the essential information required for the analysis or modeling task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "1. **Understand the Data**: Familiarize yourself with the dataset and the meaning of each feature, including price, rating, and delivery time.\n",
    "\n",
    "2. **Data Preparation**: Ensure the dataset is clean and properly structured, handling missing values and outliers if necessary.\n",
    "\n",
    "3. **Select Features**: Choose the features that are relevant for the recommendation system, such as price, rating, and delivery time.\n",
    "\n",
    "4. **Min-Max Scaling**:\n",
    "   - For each selected feature, calculate the minimum and maximum values across the dataset.\n",
    "   - Xscaled= (Xi-Xmin) / (Xmax-Xmin)\n",
    "\n",
    "5. **Scaled Data**: The scaled data will now have values between 0 and 1 for each feature, making them comparable and suitable for use in a recommendation system.\n",
    "\n",
    "6. **Model Implementation**: Depending on the specific recommendation algorithm you're using (collaborative filtering, content-based filtering, hybrid methods, etc.), incorporate the scaled features into your model.\n",
    "\n",
    "For instance, if you're using collaborative filtering, where you compare user preferences to make recommendations, the scaled features can be used to calculate similarity scores between users or items. If you're using content-based filtering, where recommendations are based on the features of the items, the scaled features can be used directly in your content-based model.\n",
    "\n",
    "By applying Min-Max scaling, you ensure that the features are on a consistent scale, preventing features with larger numerical ranges from disproportionately influencing the recommendation system. This helps in making fair and effective recommendations that consider multiple attributes like price, rating, and delivery time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "To use PCA for reducing the dimensionality of a dataset containing features for predicting stock prices, follow these steps:\n",
    "\n",
    "1. **Data Preprocessing**: Prepare and preprocess the dataset by cleaning, handling missing values, and encoding categorical features. Ensure the data is ready for analysis.\n",
    "\n",
    "2. **Standardization**: Standardize the numerical features (mean = 0, standard deviation = 1) to ensure that features with different scales don't dominate the PCA process.\n",
    "\n",
    "3. **Calculate Covariance Matrix**: Compute the covariance matrix of the standardized data. This matrix will help us understand how the features are correlated with each other.\n",
    "\n",
    "4. **Eigenvalue and Eigenvector Calculation**: Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) in which the data has the most variance, and eigenvalues indicate the amount of variance along those directions.\n",
    "\n",
    "5. **Sort Eigenvectors**: Sort the eigenvectors by their corresponding eigenvalues in descending order. This ranks the principal components based on the amount of variance they capture. Choose the top-k eigenvectors that explain a significant portion of the total variance. \n",
    "\n",
    "6. **Forming the Reduced Feature Matrix**: Construct a matrix using the top-k eigenvectors as columns. This matrix serves as the transformation matrix that maps the original features to the new reduced feature space.\n",
    "\n",
    "7. **Project Data onto Reduced Feature Space**: Multiply the original standardized data by the transformation matrix to obtain the transformed data in the reduced feature space. This data will have fewer dimensions than the original data.\n",
    "\n",
    "8. **Modeling**: Use the transformed data in the reduced feature space as input for your stock price prediction model. You can apply various machine learning algorithms, such as regression or time series models, to make predictions.\n",
    "\n",
    "9. **Evaluation**: Evaluate the performance of your model using appropriate metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or other relevant evaluation measures.\n",
    "\n",
    "It's important to note that while PCA can reduce dimensionality and potentially improve computational efficiency and generalization, it may also lead to a loss of interpretability of features. Additionally, the relationship between the reduced features and stock prices might not be as straightforward. Experiment with different numbers of principal components to find the right balance between dimensionality reduction and predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.***\n",
    "\n",
    "Xscaled= (Xi-Xmin) / (Xmax-Xmin)\n",
    "\n",
    " Xmin=1, Xmax=20\n",
    "\n",
    "for X=1 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(1-1)/(20-1)  =0\n",
    "\n",
    "for X=5 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(5-1)/(20-1)  =4/19\n",
    "\n",
    "for X=10 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(10-1)/(20-1)=9/19\n",
    "\n",
    "for X=15 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(15-1)/(20-1)=14/19\n",
    "\n",
    "for X=20 => Xscaled= (Xi-Xmin) / (Xmax-Xmin) =(20-1)/(20-1)=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "The number of principal components to retain in PCA depends on the desired trade-off between dimensionality reduction and preserving variance. A common approach is to retain enough principal components to explain around 95% to 99% of the total variance. In your case, with features [height, weight, age, gender, blood pressure], you would calculate the explained variance ratios and cumulative explained variance ratios. Based on the analysis, you could choose the number of principal components that capture a satisfactory amount of variance while maintaining interpretability and practicality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
