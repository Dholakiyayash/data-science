{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?**\n",
    "\n",
    "Ridge Regression is a variant of linear regression that introduces a regularization term to the ordinary least squares (OLS) cost function. The regularization term aims to prevent overfitting by penalizing the magnitudes of the coefficients. This technique is particularly useful when dealing with multicollinearity, which is a situation where predictors (features) in a regression model are highly correlated.\n",
    "\n",
    "The key difference between Ridge Regression and ordinary least squares regression lies in the cost function:\n",
    "\n",
    "**Ordinary Least Squares (OLS) Regression:**\n",
    "OLS minimizes the sum of squared residuals between the actual values and the predicted values. The goal is to find the coefficients that best fit the training data without any additional constraints.\n",
    "\n",
    "Cost function in OLS:\n",
    "\n",
    "j(θ) = (1/n)*​ sum_1_to_n{(yi − ^yi​ )^2}\n",
    "\n",
    "- Where:θ represents the coefficients of the predictors,\n",
    "- n is the number of observations (data points).\n",
    "- p is the number of independent variables (predictors) in the model.\n",
    "\n",
    "**Ridge Regression:**\n",
    "Ridge Regression adds a penalty term to the cost function that is proportional to the sum of squared coefficients. This penalty term, controlled by a parameter α, discourages the coefficients from taking excessively large values. As a result, Ridge encourages the model to find a balance between fitting the training data and keeping the magnitude of the coefficients in check.\n",
    "\n",
    "Cost function in Ridge Regression:\n",
    "\n",
    "j(θ) = (1/n)*​ sum_1_to_n{(yi − ^yi​ )^2} + α*sum_1_to_p(θj^2)\n",
    "\n",
    "- n is the number of data points.\n",
    "- yi is the actual output for the ith data point.\n",
    "- ^yi is the predicted output.\n",
    "- p is the number of predictors (features).\n",
    "- θ are the coefficients of the predictors.\n",
    "- α is the regularization parameter.\n",
    "\n",
    "Key points about Ridge Regression:\n",
    "\n",
    "Ridge shrinks the coefficients towards zero, but they might not become exactly zero.\n",
    "\n",
    "The regularization term α sum_1_to_p(θj^2) is added to the cost function, which adds a trade-off between model fit and complexity.\n",
    "\n",
    "The larger the value of α, the stronger the regularization effect and the more the coefficients are penalized.\n",
    "\n",
    "Ridge is especially helpful when dealing with multicollinearity, as it can help stabilize the model's behavior in the presence of highly correlated predictors.\n",
    "\n",
    "Ridge does not perform feature selection as effectively as some other methods, like Lasso regression.\n",
    "In summary, Ridge Regression adds a regularization term to the ordinary least squares cost function to prevent overfitting and improve model stability, particularly in cases of multicollinearity. It strikes a balance between fitting the data and controlling the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   j(θ) = (1/n)*​ sum_1_to_n{(yi − ^yi​ )^2}\n",
    "\n",
    "- Where:θ represents the coefficients of the predictors,\n",
    "- n is the number of observations (data points).\n",
    "- p is the number of independent variables (predictors) in the model.\n",
    "\n",
    "j(θ) = (1/n)*​ sum_1_to_n{(yi − ^yi​ )^2} + α*sum_1_to_p(θj^2)\n",
    "\n",
    "- n is the number of data points.\n",
    "- yi is the actual output for the ith data point.\n",
    "- ^yi is the predicted output.\n",
    "- p is the number of predictors (features).\n",
    "- θ are the coefficients of the predictors.\n",
    "- α is the regularization parameter."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
