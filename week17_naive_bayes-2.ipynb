{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?**\n",
    "\n",
    "- event A : he/she uses company health insurance plan.\n",
    "- event B : he/she who use plan are smokers.\n",
    "\n",
    "- P(A)=0.7 P(B and A)=0.4\n",
    "- P(B/A)=?\n",
    "\n",
    "- P(B/A) = P(B and A)/P(B)\n",
    "-        = 0.4/0.7 = 4/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**\n",
    "\n",
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes algorithm, which is a probabilistic classification method commonly used in machine learning and natural language processing tasks. They are used for text classification, spam detection, sentiment analysis, and more. Despite their similarities, they are designed for different types of data and have some key differences.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - Bernoulli Naive Bayes is used when the features (input variables) are binary (i.e., they take on values of 0 or 1).\n",
    "   - It's particularly useful for text classification tasks where the presence or absence of words in a document is used as the feature.\n",
    "   - Each feature is treated as an independent binary variable, and the assumption is that the presence or absence of one feature is independent of the presence or absence of other features given the class label.\n",
    "   - It's well-suited for tasks like sentiment analysis where the only information considered is whether a word occurs in a document or not.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - Multinomial Naive Bayes is used when the features represent discrete counts, typically representing the frequency of occurrences of certain events.\n",
    "   - It's commonly used for text classification tasks where the features are the counts of words in a document (bag-of-words representation).\n",
    "   - Unlike Bernoulli Naive Bayes, Multinomial Naive Bayes takes into account the frequency of feature occurrences rather than just their presence or absence.\n",
    "   - It's suitable for tasks where the frequency of words is important, like document classification based on word frequencies.\n",
    "\n",
    "In both cases, the \"Naive\" assumption refers to the assumption of feature independence given the class label. This assumption simplifies the computations and makes the algorithm computationally efficient, but it might not hold true for all types of data.\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the features they handle: Bernoulli is for binary features (presence or absence), and Multinomial is for discrete count features (frequency of occurrences). The choice between the two depends on the specific nature of the data and the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. How does Bernoulli Naive Bayes handle missing values?**\n",
    "\n",
    "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm that is specifically designed for binary data, where each feature represents a binary value (usually 0 or 1). This type of Naive Bayes assumes that features are conditionally independent given the class label. When it comes to handling missing values in Bernoulli Naive Bayes, there are a few strategies you can consider:\n",
    "\n",
    "1. **Ignoring Missing Values**: One common approach is to simply ignore instances with missing values during both training and classification. In Bernoulli Naive Bayes, this means excluding the instances with missing binary feature values from the calculations when estimating probabilities. While straightforward, this approach can lead to a loss of valuable information, especially if the missing values are not random.\n",
    "\n",
    "2. **Missing Values as a Separate Category**: Another strategy is to treat missing values as a separate category or state for each feature. This involves modifying the calculation of probabilities to include the missing state when estimating the likelihoods. This approach can work well when missing values are not truly missing at random and carry some meaningful information.\n",
    "\n",
    "3. **Imputation**: Imputation involves replacing missing values with estimated or imputed values. In the context of Bernoulli Naive Bayes, this could mean estimating the missing binary feature values based on the distribution of the observed values for that feature in the given class. For instance, you might impute missing values with the mode (most common value) of the observed values for that feature within the same class.\n",
    "\n",
    "4. **Using External Models**: Instead of handling missing values directly within Bernoulli Naive Bayes, you could use an external model to predict missing values and then use the completed dataset for classification. This approach can be beneficial when the relationships between features are more complex and might be better captured by a more advanced model.\n",
    "\n",
    "It's important to note that the choice of how to handle missing values depends on the nature of your data, the underlying reasons for missingness, and the performance goals of your classifier. You might need to experiment with different approaches to determine which one works best for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Can Gaussian Naive Bayes be used for multi-class classification?**\n",
    "\n",
    "Yes, Gaussian Naive Bayes can indeed be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features within each class are normally distributed. It's commonly used for continuous data, where features are real-valued.\n",
    "\n",
    "When it comes to multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by calculating the class priors and class-conditional probabilities for each feature given the class. Here's how it works:\n",
    "\n",
    "1. **Class Priors**: Calculate the prior probability of each class in the training dataset. This involves computing the proportion of instances that belong to each class.\n",
    "\n",
    "2. **Class-Conditional Probabilities**: For each feature in each class, estimate the mean and variance of the feature's values. These estimates are used to model the class-conditional probability of the feature given the class using a Gaussian (normal) distribution.\n",
    "\n",
    "3. **Classification Decision**: To classify a new instance, calculate the class posterior probabilities using Bayes' theorem, taking into account the class priors and the Gaussian distribution parameters (mean and variance) for each feature in each class. The class with the highest posterior probability is chosen as the predicted class for the instance.\n",
    "\n",
    "It's worth noting that the \"Naive\" assumption in Gaussian Naive Bayes refers to the assumption of feature independence given the class, which might not hold true for all datasets. Despite this simplification, Gaussian Naive Bayes can perform surprisingly well on various datasets, especially when the features are approximately normally distributed and the independence assumption is not severely violated.\n",
    "\n",
    "For multi-class classification, the same principles apply, but you'll extend the calculations and modeling to accommodate multiple classes rather than just two. Each class will have its own set of mean and variance parameters for each feature.\n",
    "\n",
    "However, keep in mind that Gaussian Naive Bayes might struggle when features have strong dependencies or when the data deviates significantly from the normal distribution assumption. In such cases, more advanced classifiers like Linear Discriminant Analysis (LDA) or non-parametric methods like k-Nearest Neighbors might be more suitable for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# Data preparation:\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository \n",
    "# (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the \n",
    "# goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "\n",
    "\n",
    "# 1, 0.    | spam, non-spam classes\n",
    "import pandas as pd\n",
    "columns_name=['word_freq_make',\n",
    "'word_freq_address',\n",
    "'word_freq_all',\n",
    "'word_freq_3d',\n",
    "'word_freq_our',\n",
    "'word_freq_over',\n",
    "'word_freq_remove',\n",
    "'word_freq_internet',\n",
    "'word_freq_order',\n",
    "'word_freq_mail',\n",
    "'word_freq_receive',\n",
    "'word_freq_will',\n",
    "'word_freq_people',\n",
    "'word_freq_report',\n",
    "'word_freq_addresses',\n",
    "'word_freq_free',\n",
    "'word_freq_business',\n",
    "'word_freq_email',\n",
    "'word_freq_you',\n",
    "'word_freq_credit',\n",
    "'word_freq_your',\n",
    "'word_freq_font',\n",
    "'word_freq_000',\n",
    "'word_freq_money',\n",
    "'word_freq_hp',\n",
    "'word_freq_hpl',\n",
    "'word_freq_george',\n",
    "'word_freq_650',\n",
    "'word_freq_lab',\n",
    "'word_freq_labs',\n",
    "'word_freq_telnet',\n",
    "'word_freq_857',\n",
    "'word_freq_data',\n",
    "'word_freq_415',\n",
    "'word_freq_85',\n",
    "'word_freq_technology',\n",
    "'word_freq_1999',\n",
    "'word_freq_parts',\n",
    "'word_freq_pm',\n",
    "'word_freq_direct',\n",
    "'word_freq_cs',\n",
    "'word_freq_meeting',\n",
    "'word_freq_original',\n",
    "'word_freq_project',\n",
    "'word_freq_re',\n",
    "'word_freq_edu',\n",
    "'word_freq_table',\n",
    "'word_freq_conference',\n",
    "'char_freq_;',\n",
    "'char_freq_(',\n",
    "'char_freq_[',\n",
    "'char_freq_!',\n",
    "'char_freq_$',\n",
    "'char_freq_#',\n",
    "'capital_run_length_average',\n",
    "'capital_run_length_longest',\n",
    "'capital_run_length_total',\n",
    "'spam_or_not']\n",
    "df=pd.read_csv(\"spambase/spambase.data\",names=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam_or_not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0              0.32            0.00              0.00                0.00   \n",
       "1              0.14            0.28              0.21                0.07   \n",
       "2              1.23            0.19              0.19                0.12   \n",
       "3              0.63            0.00              0.31                0.63   \n",
       "4              0.63            0.00              0.31                0.63   \n",
       "...             ...             ...               ...                 ...   \n",
       "4596           0.00            0.31              0.00                0.00   \n",
       "4597           0.00            0.00              0.00                0.00   \n",
       "4598           0.00            0.00              0.00                0.00   \n",
       "4599           0.32            0.00              0.00                0.00   \n",
       "4600           0.00            0.00              0.00                0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0                0.00            0.00  ...        0.000        0.000   \n",
       "1                0.00            0.94  ...        0.000        0.132   \n",
       "2                0.64            0.25  ...        0.010        0.143   \n",
       "3                0.31            0.63  ...        0.000        0.137   \n",
       "4                0.31            0.63  ...        0.000        0.135   \n",
       "...               ...             ...  ...          ...          ...   \n",
       "4596             0.00            0.00  ...        0.000        0.232   \n",
       "4597             0.00            0.00  ...        0.000        0.000   \n",
       "4598             0.00            0.00  ...        0.102        0.718   \n",
       "4599             0.00            0.00  ...        0.000        0.057   \n",
       "4600             0.00            0.00  ...        0.000        0.000   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0             0.0        0.778        0.000        0.000   \n",
       "1             0.0        0.372        0.180        0.048   \n",
       "2             0.0        0.276        0.184        0.010   \n",
       "3             0.0        0.137        0.000        0.000   \n",
       "4             0.0        0.135        0.000        0.000   \n",
       "...           ...          ...          ...          ...   \n",
       "4596          0.0        0.000        0.000        0.000   \n",
       "4597          0.0        0.353        0.000        0.000   \n",
       "4598          0.0        0.000        0.000        0.000   \n",
       "4599          0.0        0.000        0.000        0.000   \n",
       "4600          0.0        0.125        0.000        0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "...                          ...                         ...   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  spam_or_not  \n",
       "0                          278            1  \n",
       "1                         1028            1  \n",
       "2                         2259            1  \n",
       "3                          191            1  \n",
       "4                          191            1  \n",
       "...                        ...          ...  \n",
       "4596                        88            0  \n",
       "4597                        14            0  \n",
       "4598                       118            0  \n",
       "4599                        78            0  \n",
       "4600                        40            0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                0\n",
       "word_freq_address             0\n",
       "word_freq_all                 0\n",
       "word_freq_3d                  0\n",
       "word_freq_our                 0\n",
       "word_freq_over                0\n",
       "word_freq_remove              0\n",
       "word_freq_internet            0\n",
       "word_freq_order               0\n",
       "word_freq_mail                0\n",
       "word_freq_receive             0\n",
       "word_freq_will                0\n",
       "word_freq_people              0\n",
       "word_freq_report              0\n",
       "word_freq_addresses           0\n",
       "word_freq_free                0\n",
       "word_freq_business            0\n",
       "word_freq_email               0\n",
       "word_freq_you                 0\n",
       "word_freq_credit              0\n",
       "word_freq_your                0\n",
       "word_freq_font                0\n",
       "word_freq_000                 0\n",
       "word_freq_money               0\n",
       "word_freq_hp                  0\n",
       "word_freq_hpl                 0\n",
       "word_freq_george              0\n",
       "word_freq_650                 0\n",
       "word_freq_lab                 0\n",
       "word_freq_labs                0\n",
       "word_freq_telnet              0\n",
       "word_freq_857                 0\n",
       "word_freq_data                0\n",
       "word_freq_415                 0\n",
       "word_freq_85                  0\n",
       "word_freq_technology          0\n",
       "word_freq_1999                0\n",
       "word_freq_parts               0\n",
       "word_freq_pm                  0\n",
       "word_freq_direct              0\n",
       "word_freq_cs                  0\n",
       "word_freq_meeting             0\n",
       "word_freq_original            0\n",
       "word_freq_project             0\n",
       "word_freq_re                  0\n",
       "word_freq_edu                 0\n",
       "word_freq_table               0\n",
       "word_freq_conference          0\n",
       "char_freq_;                   0\n",
       "char_freq_(                   0\n",
       "char_freq_[                   0\n",
       "char_freq_!                   0\n",
       "char_freq_$                   0\n",
       "char_freq_#                   0\n",
       "capital_run_length_average    0\n",
       "capital_run_length_longest    0\n",
       "capital_run_length_total      0\n",
       "spam_or_not                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation:\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using \n",
    "# the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each \n",
    "# classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Mean Accuracy: 0.8839380364047911\n",
      "Multinomial Naive Bayes Mean Accuracy: 0.7863496180326323\n",
      "Gaussian Naive Bayes Mean Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "#  Create instances of the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate accuracy scores for each classifier\n",
    "# You can use a different scoring metric if needed (e.g., precision, recall, F1-score)\n",
    "num_folds = 10\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=num_folds)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=num_folds)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=num_folds)\n",
    "\n",
    "# Calculate and print mean accuracy scores for each classifier\n",
    "print(\"Bernoulli Naive Bayes Mean Accuracy:\", np.mean(bernoulli_scores))\n",
    "print(\"Multinomial Naive Bayes Mean Accuracy:\", np.mean(multinomial_scores))\n",
    "print(\"Gaussian Naive Bayes Mean Accuracy:\", np.mean(gaussian_scores))\n",
    "\n",
    "\n",
    "bernoulli=BernoulliNB()\n",
    "multinomial=MultinomialNB()\n",
    "gaussian=GaussianNB()\n",
    "\n",
    "# fit train data\n",
    "bernoulli.fit(X_train_scaled,y_train)\n",
    "multinomial.fit(X_train,y_train)\n",
    "gaussian.fit(X_train_scaled,y_train)\n",
    "\n",
    "# rpedict test data\n",
    "y_pred_bernoulli=bernoulli.predict(X_test_scaled)\n",
    "y_pred_multinomial=multinomial.predict(X_test)\n",
    "y_pred_gaussian=gaussian.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoullt accuracy : 0.6147719044170891\n",
      "multinomial accuracy : 0.6191165821868212\n",
      "gaussian accuracy : 0.44822592324402605\n",
      "\n",
      "bernoulli precision : 0.9440298507462687\n",
      "multinomial precision : 0.8445273631840796\n",
      "gaussian precision : 0.7288557213930348\n",
      "\n",
      "recall bernoulli : 0.8939929328621908\n",
      "recall multinomial : 0.7941520467836257\n",
      "recall gaussian : 0.9466882067851373\n",
      "\n",
      "f1 score of bernoulli : 0.9183303085299456\n",
      "f1 score of multinomial : 0.818565400843882\n",
      "f1 score of gaussian : 0.8236120871398454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results:\n",
    "# Report the following performance metrics for each classifier:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat=confusion_matrix(y_test,y_pred_bernoulli)\n",
    "tp_bernoulli = conf_mat[0][0]\n",
    "fp_bernoulli = conf_mat[0][1]\n",
    "fn_bernoulli = conf_mat[1][0]\n",
    "tn_bernoulli = conf_mat[1][1]\n",
    "\n",
    "conf_mat=confusion_matrix(y_test,y_pred_multinomial)\n",
    "tp_multinomial = conf_mat[0][0]\n",
    "fp_multinomial = conf_mat[0][1]\n",
    "fn_multinomial = conf_mat[1][0]\n",
    "tn_multinomial = conf_mat[1][1]\n",
    "\n",
    "conf_mat=confusion_matrix(y_test,y_pred_gaussian)\n",
    "tp_gaussian = conf_mat[0][0]\n",
    "fp_gaussian = conf_mat[0][1]\n",
    "fn_gaussian = conf_mat[1][0]\n",
    "tn_gaussian = conf_mat[1][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "# accuracy=(true_positive + false_negative) / (true_positive + true_negative + false_positive + false_negative )\n",
    "# accuracy\n",
    "\n",
    "print(\"bernoullt accuracy :\",(tp_bernoulli+fn_bernoulli)/(tp_bernoulli+tn_bernoulli+fp_bernoulli+fn_bernoulli))\n",
    "print(\"multinomial accuracy :\",(tp_multinomial+fn_multinomial)/(tp_multinomial+tn_multinomial+fp_multinomial+fn_multinomial))\n",
    "print(\"gaussian accuracy :\",(tp_gaussian+fn_gaussian)/(tp_gaussian+tn_gaussian+fp_gaussian+fn_gaussian))\n",
    "print()\n",
    "# Precision\n",
    "# precision = TP / (TP + FP)\n",
    "precision_bernoulli=tp_bernoulli/(tp_bernoulli+fp_bernoulli)\n",
    "precision_multinomial=tp_multinomial/(tp_multinomial+fp_multinomial)\n",
    "precision_gaussian = tp_gaussian/(tp_gaussian+fp_gaussian)\n",
    "print('bernoulli precision :',precision_bernoulli)\n",
    "print('multinomial precision :',precision_multinomial)\n",
    "print('gaussian precision :',precision_gaussian)\n",
    "print()\n",
    "\n",
    "# Recall\n",
    "# recall = TP / (TP + FN)\n",
    "recall_bernoulli=tp_bernoulli/(tp_bernoulli+fn_bernoulli)\n",
    "recall_multinomial=tp_multinomial/(tp_multinomial+fn_multinomial)\n",
    "recall_gaussian=tp_gaussian/(tp_gaussian+fn_gaussian)\n",
    "print('recall bernoulli :',recall_bernoulli)\n",
    "print('recall multinomial :',recall_multinomial)\n",
    "print('recall gaussian :',recall_gaussian)\n",
    "print()\n",
    "\n",
    "# F1 score\n",
    "# F1_Score=2*(recall * precision) / (recall + precision)\n",
    "f1_bernoulli=2*(recall_bernoulli * precision_bernoulli)/(recall_bernoulli+precision_bernoulli)\n",
    "f1_multinomial=2*(recall_multinomial * precision_multinomial)/(recall_multinomial+precision_multinomial)\n",
    "f1_gaussian=2*(recall_gaussian * precision_gaussian)/(recall_gaussian+precision_gaussian)\n",
    "print('f1 score of bernoulli :',f1_bernoulli)\n",
    "print('f1 score of multinomial :',f1_multinomial)\n",
    "print('f1 score of gaussian :',f1_gaussian)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "**Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?**\n",
    "\n",
    "- Bernoulli Naive Bayes performed the best in terms of accuracy, precision, recall, and F1-score among the three variants. It achieved the highest accuracy, indicating that it made the fewest  is classifications on the test data.\n",
    "\n",
    "- Multinomial Naive Bayes performed slightly better than Gaussian Naive Bayes in terms of accuracy. It is commonly used for text classification tasks when dealing with discrete data (e.g., word counts).\n",
    "\n",
    "- Gaussian Naive Bayes performed the worst among the three variants. This is not surprising because Gaussian Naive Bayes is designed for continuous data, and your dataset seems to have many binary features (possibly word presence or absence in text data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "**Summarise your findings and provide some suggestions for future work.**\n",
    "\n",
    "n summary, the choice of the best Naive Bayes variant depends on the nature of your data. In this case, Bernoulli Naive Bayes performed the best, likely due to the binary nature of your features. However, it's essential to consider the assumptions and limitations of Naive Bayes when applying it to your specific problem. For more complex relationships and high-dimensional data, other classifiers may be more suitable.\n",
    "\n",
    "Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
    "link through your dashboard. Make sure the repository is public.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
