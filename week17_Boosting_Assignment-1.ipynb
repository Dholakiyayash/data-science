{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is boosting in machine learning?**\n",
    "\n",
    "Boosting is an ensemble machine learning technique that combines the predictions of multiple weak models to create a strong and accurate predictive model. It focuses on correcting errors made by previous models by giving more weight to misclassified data points, resulting in improved overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What are the advantages and limitations of using boosting techniques?**\n",
    "\n",
    "**Advantages of Boosting Techniques:**\n",
    "\n",
    "1. **High Accuracy:** Boosting often produces highly accurate models, making it suitable for challenging classification and regression tasks.\n",
    "\n",
    "2. **Reduces Bias:** Boosting effectively reduces bias by focusing on correcting errors, resulting in models that can capture complex relationships in the data.\n",
    "\n",
    "3. **Handles Imbalanced Data:** Boosting can handle imbalanced datasets by assigning more weight to minority class samples, improving their representation in the ensemble.\n",
    "\n",
    "4. **Feature Importance:** Many boosting algorithms provide feature importance scores, helping identify the most relevant features in the data.\n",
    "\n",
    "5. **Versatile:** Boosting can be applied to various machine learning algorithms, making it versatile in model selection.\n",
    "\n",
    "**Limitations of Boosting Techniques:**\n",
    "\n",
    "1. **Sensitivity to Noisy Data:** Boosting can be sensitive to noisy or outlier data points, which may lead to overfitting.\n",
    "\n",
    "2. **Complexity:** Boosting algorithms can be computationally expensive and may require careful hyperparameter tuning.\n",
    "\n",
    "3. **Potential Overfitting:** Without proper regularization, boosting can lead to overfitting, especially when the number of weak learners (iterations) is too high.\n",
    "\n",
    "4. **Interpretability:** Boosting models can be less interpretable than individual weak learners, as they combine multiple models into one.\n",
    "\n",
    "5. **Data Size:** Boosting may require a sufficient amount of data to perform well, and it may not be ideal for very small datasets.\n",
    "\n",
    "6. **Risk of Underfitting:** In some cases, boosting can focus too much on correcting errors, leading to underfitting when errors are rare and real patterns are overlooked.\n",
    "\n",
    "In summary, boosting techniques are powerful for improving model accuracy and reducing bias, but they require careful handling of noisy data and hyperparameter tuning to avoid overfitting. Their computational cost should also be considered, especially in large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Explain how boosting works.**\n",
    "\n",
    "Boosting is an ensemble machine learning technique that works by combining the predictions of multiple weak learners (typically simple models) to create a single strong learner. The key idea behind boosting is to sequentially train weak learners, giving more weight to data points that are misclassified or poorly predicted by the current ensemble. Here's how boosting works step by step:\n",
    "\n",
    "1. **Initialize Weights:** Initially, each training instance is assigned an equal weight. These weights determine the importance of each instance during the learning process.\n",
    "\n",
    "2. **Train a Weak Learner:** A weak learner is trained on the weighted training data. The goal is to create a model that performs slightly better than random guessing. Examples of weak learners include decision stumps (shallow decision trees), linear models, or even simple rules.\n",
    "\n",
    "3. **Make Predictions:** The trained weak learner makes predictions on the training data.\n",
    "\n",
    "4. **Compute Errors:** Errors are calculated by comparing the weak learner's predictions to the true target values. Instances that are misclassified or poorly predicted receive higher error values.\n",
    "\n",
    "5. **Update Weights:** The weights of training instances are adjusted based on the errors made by the current weak learner. Misclassified instances receive higher weights, while correctly classified instances receive lower weights. This adjustment focuses the next weak learner on the challenging data points.\n",
    "\n",
    "6. **Train the Next Weak Learner:** A new weak learner is trained on the updated weighted data, with the goal of correcting the errors made by the previous learner.\n",
    "\n",
    "7. **Repeat:** Steps 3 to 6 are repeated for a predefined number of iterations or until a stopping criterion is met. Each new learner aims to minimize the total error made by the ensemble of weak learners.\n",
    "\n",
    "8. **Combine Predictions:** The final prediction is obtained by combining the predictions of all weak learners. Typically, a weighted or majority voting scheme is used to make the final decision.\n",
    "\n",
    "Boosting algorithms, such as AdaBoost (Adaptive Boosting) and Gradient Boosting, differ in their strategies for updating weights, selecting weak learners, and combining predictions. Gradient Boosting, for example, minimizes the error of the previous model by fitting new models to the residuals (the differences between predictions and actual values).\n",
    "\n",
    "The process of boosting continues until a specified number of weak learners are trained, or until the model achieves satisfactory performance on the training data. The resulting ensemble model is often much more accurate than any of the individual weak learners and is robust in handling complex relationships in the data.\n",
    "\n",
    "In summary, boosting works by iteratively training weak learners, adjusting instance weights to focus on challenging data points, and combining the predictions of weak learners to create a strong and accurate predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. What are the different types of boosting algorithms?**\n",
    "\n",
    "Boosting is a versatile ensemble technique, and several boosting algorithms have been developed over the years. Each of these algorithms has unique characteristics and strategies for improving model accuracy. Here are some of the different types of boosting algorithms:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most well-known boosting algorithms. It assigns weights to training instances, giving more weight to misclassified instances. Weak learners are trained sequentially on the weighted data to correct errors made by previous learners. AdaBoost is effective for classification tasks.\n",
    "\n",
    "2. **Gradient Boosting:** Gradient Boosting is a generic boosting algorithm that minimizes the error of the previous model at each iteration. It fits new weak learners to the residuals (the differences between predictions and actual values) of the current model. Popular implementations of gradient boosting include XGBoost, LightGBM, and CatBoost, which optimize various aspects of the algorithm.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):** XGBoost is an efficient and scalable gradient boosting framework known for its speed and performance. It includes regularized learning objectives, tree pruning, and parallelization, making it a popular choice for both classification and regression tasks.\n",
    "\n",
    "4. **LightGBM:** LightGBM is another gradient boosting framework designed for efficiency and speed. It uses histogram-based learning and parallel training to handle large datasets. LightGBM is suitable for both classification and regression problems.\n",
    "\n",
    "5. **CatBoost:** CatBoost is a gradient boosting algorithm designed to handle categorical features efficiently. It automatically encodes categorical variables and includes regularization techniques to prevent overfitting. CatBoost is well-suited for tasks with mixed data types.\n",
    "\n",
    "6. **Stochastic Gradient Boosting:** This variant of gradient boosting introduces stochasticity by randomly sampling subsets of data during each iteration. This helps improve generalization and reduces overfitting.\n",
    "\n",
    "7. **LogitBoost:** LogitBoost is a boosting algorithm specifically designed for binary classification tasks. It minimizes a logistic loss function and updates the model's parameters accordingly.\n",
    "\n",
    "8. **BrownBoost:** BrownBoost is an extension of AdaBoost that combines the strength of both boosting and bagging. It aims to improve robustness and accuracy by iteratively boosting and bagging weak learners.\n",
    "\n",
    "9. **LPBoost (Linear Programming Boosting):** LPBoost focuses on minimizing the margin between classes in binary classification problems. It formulates boosting as a linear programming problem and updates the classifier's coefficients.\n",
    "\n",
    "10. **MadaBoost (Multi-class AdaBoost):** MadaBoost is an extension of AdaBoost for multi-class classification tasks. It uses a reduction-to-binary approach to handle multiple classes.\n",
    "\n",
    "These are some of the most well-known and widely used boosting algorithms. The choice of which algorithm to use often depends on the specific problem, dataset size, and desired performance characteristics. Researchers and practitioners continue to develop new boosting variants and improvements to enhance model accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. What are some common parameters in boosting algorithms?**\n",
    "\n",
    "Boosting algorithms come with a variety of parameters that allow you to fine-tune and control the behavior of the algorithm. Here are some common parameters that you might encounter in boosting algorithms like AdaBoost, Gradient Boosting (including XGBoost, LightGBM, and CatBoost), and others:\n",
    "\n",
    "1. **Number of Estimators (n_estimators):** This parameter determines the number of weak learners (base models) that are trained and combined in the ensemble. Increasing the number of estimators can improve accuracy but also increase training time.\n",
    "\n",
    "2. **Learning Rate (or Shrinkage):** The learning rate controls the step size at which the algorithm updates the model parameters. Smaller values (e.g., 0.01) make the learning process more cautious, while larger values (e.g., 0.1) make it faster but riskier in terms of overfitting.\n",
    "\n",
    "3. **Base Learner:** You can typically choose from various base learners, such as decision stumps (shallow trees), linear models, or even more complex models. The choice of the base learner depends on the problem and dataset.\n",
    "\n",
    "4. **Max Depth (max_depth):** If decision trees are used as base learners, this parameter sets the maximum depth of the trees. Limiting tree depth helps prevent overfitting.\n",
    "\n",
    "5. **Subsample (or Fraction of Samples):** This parameter controls the fraction of training data randomly sampled at each iteration. It introduces stochasticity and can help improve generalization.\n",
    "\n",
    "6. **Subsample Features (colsample_bytree or colsample_bylevel):** For tree-based algorithms, these parameters control the fraction of features randomly selected at each iteration. They add randomness to feature selection, which can reduce overfitting.\n",
    "\n",
    "7. **Regularization Parameters (e.g., alpha, lambda):** Some boosting algorithms include L1 (Lasso) and L2 (Ridge) regularization terms to prevent overfitting by penalizing complex models.\n",
    "\n",
    "8. **Loss Function:** Boosting algorithms allow you to specify different loss functions based on the task (e.g., logistic loss for classification, mean squared error for regression). Some algorithms also provide custom loss functions.\n",
    "\n",
    "9. **Early Stopping:** Early stopping mechanisms allow you to stop the training process when a certain condition is met, such as no improvement in validation performance for a specified number of iterations.\n",
    "\n",
    "10. **Feature Importance:** Boosting algorithms often provide feature importance scores that indicate the contribution of each feature to the model's predictions. You can set thresholds to select the most important features.\n",
    "\n",
    "11. **Cross-Validation Parameters:** These parameters control the cross-validation strategy, such as the number of folds and whether to use stratified sampling (for classification).\n",
    "\n",
    "12. **Parallelization:** Some boosting implementations support parallel training, allowing you to utilize multiple CPU cores or GPUs for faster training.\n",
    "\n",
    "13. **Hyperparameter Search:** Techniques like grid search or randomized search can help you find the best combination of hyperparameters for your specific problem.\n",
    "\n",
    "14. **Random Seed:** Setting a random seed ensures reproducibility of results, as boosting algorithms introduce randomness during training.\n",
    "\n",
    "15. **Objective Function (for custom loss functions):** If you define custom loss functions, you may need to specify how the boosting algorithm optimizes the objective function.\n",
    "\n",
    "16. **Class Weights (for imbalanced datasets):** Some boosting algorithms allow you to assign different weights to classes to address class imbalance issues.\n",
    "\n",
    "These parameters may vary slightly between different boosting implementations, so it's essential to consult the documentation and user guides for the specific library or framework you're using. Tuning these parameters carefully is often necessary to achieve the best model performance for your particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. How do boosting algorithms combine weak learners to create a strong learner?**\n",
    "\n",
    "Boosting algorithms combine weak learners to create a strong learner through a weighted or adaptive aggregation scheme. The key idea is to give more importance to the predictions of weak learners that perform well on the training data and less importance to those that perform poorly. Here's how boosting algorithms typically combine weak learners:\n",
    "\n",
    "1. **Initialization:** Initially, each data point in the training set is assigned an equal weight (or probability).\n",
    "\n",
    "2. **Training Weak Learners:** Boosting starts by training a weak learner on the weighted training data. This weak learner could be a simple model like a decision stump (a shallow decision tree with one split), a linear model, or any other suitable choice.\n",
    "\n",
    "3. **Predictions:** After training, the weak learner makes predictions on the training data.\n",
    "\n",
    "4. **Weighted Error Calculation:** The boosting algorithm calculates the weighted error of the weak learner's predictions. This error measure reflects how well or poorly the weak learner performs on the training data. Data points that are misclassified or poorly predicted receive higher weights.\n",
    "\n",
    "5. **Calculate Weak Learner Weight:** The boosting algorithm calculates a weight for the weak learner based on its performance. Good learners receive higher weights, indicating that their predictions are more reliable.\n",
    "\n",
    "6. **Update Weights:** The algorithm updates the weights of data points based on their misclassification or error. Instances that were incorrectly predicted by the current weak learner receive higher weights, making them more important for the subsequent learner. Correctly predicted instances receive lower weights.\n",
    "\n",
    "7. **Repeat:** Steps 2 to 6 are repeated for a predefined number of iterations (or until a stopping criterion is met). At each iteration, a new weak learner is trained, and the weights of training instances are adjusted.\n",
    "\n",
    "8. **Final Prediction:** To make a final prediction, the boosting algorithm combines the predictions of all weak learners. The combination can involve weighted averaging or voting, where the predictions of weak learners with higher weights have more influence on the final decision.\n",
    "\n",
    "9. **Weighted Averaging (Regression) or Weighted Voting (Classification):** In regression tasks, boosting often uses weighted averaging of weak learners' predictions to produce the final prediction. In classification tasks, weighted voting is used, where each weak learner's vote is weighted by its performance.\n",
    "\n",
    "By iteratively training weak learners and adjusting instance weights, boosting focuses on difficult-to-classify or poorly predicted data points. It effectively \"learns\" from its mistakes and gradually assembles a strong learner that can generalize well to unseen data. The final ensemble model tends to perform significantly better than individual weak learners and can capture complex relationships in the data.\n",
    "\n",
    "Popular boosting algorithms, such as AdaBoost, Gradient Boosting (including XGBoost, LightGBM, and CatBoost), and others, follow variations of this general process to combine weak learners effectively. The specific strategies for updating weights and selecting the best learners may vary between these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Explain the concept of AdaBoost algorithm and its working.**\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is one of the earliest and most influential boosting algorithms used for binary classification tasks. It works by combining multiple weak learners (often simple decision stumps) to create a strong and accurate classifier. Here's how AdaBoost algorithm works:\n",
    "\n",
    "1. **Initialization:** Each training instance is initially assigned equal weight, meaning that all data points have the same importance.\n",
    "\n",
    "2. **Sequential Training of Weak Learners:** AdaBoost iteratively trains a series of weak learners, typically decision stumps. These are simple models that make binary decisions based on a single feature. Each weak learner is trained to minimize the weighted classification error, focusing on the data points that were misclassified by the previous learners.\n",
    "\n",
    "3. **Weighted Training Data:** At each iteration, the training data is weighted based on the correctness of the predictions made by the previous weak learners. Data points that were incorrectly classified receive higher weights, making them more important for the current weak learner.\n",
    "\n",
    "4. **Weak Learner's Vote:** After training each weak learner, AdaBoost calculates a weight (alpha) for that learner based on its performance. The better the learner's performance, the higher its weight. This weight indicates how much influence the learner's prediction will have on the final classification.\n",
    "\n",
    "5. **Updating Instance Weights:** AdaBoost updates the weights of training instances based on the correctness of predictions made by the current weak learner. Instances that were misclassified by the learner receive higher weights, while correctly classified instances receive lower weights. This updating process focuses the next learner on the challenging data points.\n",
    "\n",
    "6. **Combining Predictions:** To make a final prediction, AdaBoost combines the predictions of all weak learners. The combination typically involves weighted voting, where each weak learner's vote is weighted by its alpha value. Weak learners with higher alpha values have more influence on the final decision.\n",
    "\n",
    "7. **Final Classification:** The ensemble model created by AdaBoost is the weighted combination of weak learners' decisions. It assigns the class label that receives the most weighted votes as the final prediction.\n",
    "\n",
    "8. **Boosting Iterations:** AdaBoost repeats this process for a predefined number of iterations or until a stopping criterion is met. Each new weak learner is trained to correct the errors made by the previous learners.\n",
    "\n",
    "AdaBoost's strength lies in its ability to focus on difficult-to-classify data points and adapt to the complexity of the problem. As the iterations progress, it assigns more importance to instances that are challenging to classify, which helps in improving the overall classification accuracy. It effectively constructs a strong classifier by emphasizing the features and examples that matter most.\n",
    "\n",
    "One of AdaBoost's important properties is that it can be combined with any weak learner, as long as the learner performs slightly better than random guessing. AdaBoost also provides feature importance scores, helping to identify the most relevant features in the data.\n",
    "\n",
    "However, AdaBoost is sensitive to noisy data and outliers, which can lead to overfitting. It's essential to preprocess the data and choose an appropriate base learner to make AdaBoost effective. Despite its limitations, AdaBoost has been influential in the development of boosting techniques and remains a valuable tool in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. What is the loss function used in AdaBoost algorithm?**\n",
    "\n",
    "The AdaBoost (Adaptive Boosting) algorithm does not use a traditional loss function like many other machine learning algorithms. Instead, it employs a different concept to determine the weight of each weak learner (base model) and the importance of each training instance. Here's how this concept works in AdaBoost:\n",
    "\n",
    "1. **Weighted Classification Error:** AdaBoost uses a weighted classification error measure to assess the performance of each weak learner during training. The weighted error considers both the correctness of predictions and the importance of the training instances.\n",
    "\n",
    "2. **Classifier's Weight (Alpha):** After each iteration, AdaBoost assigns a weight (usually denoted as alpha, α) to the weak learner based on its performance. The better the learner's performance, the higher its weight (alpha). Weak learners that perform well are given more influence in the final prediction.\n",
    "\n",
    "3. **Instance Weights:** The weights of the training instances are updated at each iteration based on the correctness of predictions made by the current weak learner. Instances that are misclassified by the learner receive higher weights, while correctly classified instances receive lower weights. This adjustment helps the algorithm focus on challenging data points.\n",
    "\n",
    "4. **Combining Predictions:** AdaBoost combines the predictions of all weak learners by taking a weighted vote. The alpha values of the learners determine how much influence each learner's prediction has on the final classification. Learners with higher alpha values contribute more to the final decision.\n",
    "\n",
    "The absence of a traditional loss function in AdaBoost is a unique aspect of the algorithm. Instead of explicitly minimizing a loss function like gradient boosting (which minimizes mean squared error, log loss, etc.), AdaBoost focuses on updating instance weights and learner weights to improve classification accuracy. The weighted error calculation and alpha values are integral to AdaBoost's ability to adapt and emphasize challenging data points during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9. How does the AdaBoost algorithm update the weights of misclassified samples?**\n",
    "\n",
    "The AdaBoost (Adaptive Boosting) algorithm updates the weights of misclassified samples to give them more importance during subsequent iterations. This weight adjustment is a key aspect of AdaBoost's strategy to focus on challenging data points and improve the accuracy of the ensemble model. Here's how the algorithm updates the weights of misclassified samples:\n",
    "\n",
    "1. **Initialization:** Initially, all training samples are assigned equal weights. Each sample has an equal probability of being selected during training.\n",
    "\n",
    "2. **Weighted Error Calculation:** After each iteration (each weak learner's training phase), AdaBoost calculates the weighted error (weighted misclassification rate) of the current weak learner. The weighted error considers both the correctness of the learner's predictions and the importance (weights) of the training samples.\n",
    "\n",
    "3. **Calculation of Alpha (α):** AdaBoost calculates an alpha value (α) for the current weak learner based on its weighted error. The formula for alpha is typically as follows:\n",
    "\n",
    "   **α = 0.5 * ln((1 - weighted_error) / weighted_error)**\n",
    "\n",
    "   - If the weighted error is low (the weak learner performs well), alpha is high, indicating that this learner's predictions should have more influence on the final classification.\n",
    "   - If the weighted error is high (the weak learner performs poorly), alpha is low, indicating that this learner's predictions should have less influence.\n",
    "\n",
    "4. **Weight Adjustment:** AdaBoost updates the weights of the training samples for the next iteration. Misclassified samples receive increased weights, while correctly classified samples receive decreased weights. The weight update for each sample is calculated as follows:\n",
    "\n",
    "   **For misclassified sample i:**\n",
    "   - New Weight (t+1) = Old Weight (t) * exp(α)\n",
    "   \n",
    "   **For correctly classified sample i:**\n",
    "   - New Weight (t+1) = Old Weight (t) * exp(-α)\n",
    "\n",
    "   Here, \"t\" represents the current iteration.\n",
    "\n",
    "   - The exponential term (exp(α) or exp(-α)) adjusts the weights proportionally to the performance of the current weak learner. If a sample was misclassified, its weight increases, making it more likely to be selected in the next iteration. Conversely, if a sample was correctly classified, its weight decreases.\n",
    "\n",
    "5. **Normalization:** After updating the weights, AdaBoost normalizes them to ensure that they sum to one. This step maintains the weights as probabilities, making the sampling process consistent.\n",
    "\n",
    "6. **Repeat:** Steps 2 to 5 are repeated for a predefined number of iterations (or until a stopping criterion is met). Each new weak learner focuses on the challenging data points by giving higher importance to misclassified samples.\n",
    "\n",
    "By iteratively increasing the weights of misclassified samples and decreasing the weights of correctly classified samples, AdaBoost ensures that the ensemble learns to focus on the instances that are difficult to classify. This emphasis on challenging data points allows AdaBoost to construct a strong classifier that adapts well to the complexity of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?**\n",
    "\n",
    "Increasing the number of estimators (weak learners or base models) in the AdaBoost algorithm can have both positive and negative effects on the model's performance and training time. Here's what happens when you increase the number of estimators in AdaBoost:\n",
    "\n",
    "**Positive Effects:**\n",
    "\n",
    "1. **Improved Accuracy:** As you increase the number of estimators, AdaBoost has more opportunities to refine its predictions. It can gradually correct errors made by previous weak learners, leading to improved classification accuracy on both the training and test data.\n",
    "\n",
    "2. **Better Generalization:** A larger number of estimators can help AdaBoost generalize better to unseen data. It reduces the risk of overfitting, as the ensemble becomes more stable and less prone to capturing noise in the training data.\n",
    "\n",
    "3. **Increased Robustness:** With more estimators, AdaBoost can better handle noisy or complex datasets. It becomes more capable of capturing complex decision boundaries and adapting to various data distributions.\n",
    "\n",
    "**Negative Effects:**\n",
    "\n",
    "1. **Increased Training Time:** Adding more estimators results in training more weak learners, which can significantly increase the training time. Training many weak learners can be computationally expensive, especially if each weak learner is complex.\n",
    "\n",
    "2. **Diminishing Returns:** There are diminishing returns in terms of accuracy as you add more estimators. Initially, adding more estimators leads to substantial improvements in accuracy, but after a certain point, the gains become marginal, and the algorithm may plateau.\n",
    "\n",
    "3. **Risk of Overfitting (in Extreme Cases):** While AdaBoost is less prone to overfitting compared to some other algorithms, there's still a risk of overfitting if you add an excessive number of estimators. The ensemble might start fitting the noise in the training data.\n",
    "\n",
    "The optimal number of estimators in AdaBoost depends on the specific problem and dataset. You often need to perform model selection, such as cross-validation or using a validation dataset, to find the right balance between accuracy and training time. A common approach is to monitor the accuracy on a validation set as you incrementally increase the number of estimators and stop when the performance stabilizes or starts to degrade.\n",
    "\n",
    "It's worth noting that AdaBoost is often used with a relatively small number of estimators (e.g., 50 to 500) to strike a balance between accuracy and efficiency. However, the ideal number can vary depending on the complexity of the problem and the quality of the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
